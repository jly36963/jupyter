{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "# linalg\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='white', color_codes=True)\n",
    "%matplotlib inline\n",
    "\n",
    "# pre-processing\n",
    "from sklearn.pipeline import make_pipeline  \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split # train test split\n",
    "\n",
    "# decomposition\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# supervised algorithms\n",
    "from sklearn.linear_model import LinearRegression # linear regression\n",
    "from sklearn.linear_model import RANSACRegressor # linear regression\n",
    "from sklearn.linear_model import LogisticRegression #logistic regression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm #support vector Machine\n",
    "from sklearn.ensemble import RandomForestClassifier #Random Forest\n",
    "from sklearn.neighbors import KNeighborsClassifier #KNN\n",
    "from sklearn.naive_bayes import GaussianNB #Naive bayes\n",
    "from sklearn.tree import DecisionTreeClassifier #Decision Tree\n",
    "\n",
    "# artificial/simulated data\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# metrics\n",
    "from sklearn import metrics #accuracy measure\n",
    "from sklearn.metrics import confusion_matrix #for confusion matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# cross validation\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# misc\n",
    "import sys,os\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# to show images\n",
    "def show_image(fn, width=500):\n",
    "    fp1 = os.path.join('./diagrams/', fn)\n",
    "    display(Image(filename=fp1, width=width))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np: 1.17.3\n",
      "pd: 0.25.2\n",
      "sns: 0.9.0\n",
      "py: (3, 7, 3)\n"
     ]
    }
   ],
   "source": [
    "# versions\n",
    "print(f\"np: {np.__version__}\")\n",
    "print(f\"pd: {pd.__version__}\")\n",
    "print(f\"sns: {sns.__version__}\")\n",
    "print(f\"py: {sys.version_info[0:3]}\") # sys.version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaggle titanic example (pt1)\n",
    "    # https://www.kaggle.com/aaysbt/titanic-datasets-eda-fe-dc-model-predictions\n",
    "    # did the passenger survive?\n",
    "    \n",
    "train = pd.read_csv('./data/titanic_train.csv')\n",
    "test = pd.read_csv('./data/titanic_test.csv')\n",
    "\n",
    "# -----------\n",
    "# data preperation steps\n",
    "# -----------\n",
    "\n",
    "# fill in missing values (imputation)\n",
    "# impute age\n",
    "    # determine average age, based on class\n",
    "    # fill in null values with associated averages\n",
    "def impute_age(cols):\n",
    "    Age = cols[0]\n",
    "    Pclass = cols[1]\n",
    "    if pd.isnull(Age):\n",
    "        if Pclass == 1:\n",
    "            return 37\n",
    "        elif Pclass == 2:\n",
    "            return 29\n",
    "        elif Pclass == 3:\n",
    "            return 24\n",
    "        else:\n",
    "            return 30 # this fallback value is arbitrary. do better.\n",
    "    else: return Age\n",
    "# fill in 'age'\n",
    "train['Age'] = train[['Age', 'Pclass']].apply(impute_age, axis=1)\n",
    "# remove 'cabin'\n",
    "train = train.drop('Cabin', axis=1)\n",
    "# remove null rows (only a handful left)\n",
    "train = train.dropna()\n",
    "# convert categories into dummy values\n",
    "    # drop_first should be true\n",
    "    # http://www.statsmodels.org/dev/contrasts.html\n",
    "sex = pd.get_dummies(train['Sex'], drop_first=True)\n",
    "embark = pd.get_dummies(train['Embarked'], drop_first=True)\n",
    "# replace categorical columns with dummy columns\n",
    "train = pd.concat([train, sex, embark], axis=1)\n",
    "# drop unusable columns\n",
    "train = train.drop(['Sex', 'Embarked', 'Name', 'Ticket'], axis=1)\n",
    "\n",
    "# split\n",
    "y = train.Survived\n",
    "X = train.drop('Survived', axis=1)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression accuracy: 0.81\n",
      "linear SVM accuracy: 0.79\n",
      "radial SVM accuracy: 0.6\n",
      "decision tree accuracy: 0.8\n",
      "knn accuracy: 0.66\n",
      "gaussian naive bayes accuracy: 0.8\n",
      "random forest accuracy: 0.85\n"
     ]
    }
   ],
   "source": [
    "# kaggle titanic example (pt2)\n",
    "\n",
    "# -----------\n",
    "# classification\n",
    "# -----------\n",
    "    \n",
    "# logistic regression\n",
    "logr = LogisticRegression(solver='liblinear')\n",
    "logr.fit(X_train, y_train)\n",
    "y_pred = logr.predict(X_valid)\n",
    "acc = metrics.accuracy_score(y_pred, y_valid)\n",
    "print(f\"logistic regression accuracy: {round(acc,2)}\")\n",
    "\n",
    "# linear SVM\n",
    "    # https://medium.com/@ankitnitjsr13/math-behind-support-vector-machine-svm-5e7376d0ee4d\n",
    "model_svm_l = svm.SVC(kernel='linear', C=0.1, gamma=0.1)\n",
    "model_svm_l.fit(X_train, y_train)\n",
    "y_pred = model_svm_l.predict(X_valid)\n",
    "acc = metrics.accuracy_score(y_pred, y_valid)\n",
    "print(f\"linear SVM accuracy: {round(acc,2)}\")\n",
    "\n",
    "# radial SVM\n",
    "model_svm_rbf = svm.SVC(kernel='rbf', C=0.1, gamma=0.1)\n",
    "model_svm_rbf.fit(X_train, y_train)\n",
    "y_pred = model_svm_rbf.predict(X_valid)\n",
    "acc = metrics.accuracy_score(y_pred, y_valid)\n",
    "print(f\"radial SVM accuracy: {round(acc,2)}\")\n",
    "\n",
    "# decision tree\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred = tree.predict(X_valid)\n",
    "acc = metrics.accuracy_score(y_pred, y_valid)\n",
    "print(f\"decision tree accuracy: {round(acc,2)}\")\n",
    "\n",
    "# K nearest neighbors (KNN)\n",
    "knn = KNeighborsClassifier(n_neighbors=4) # most accurate in her example\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_valid)\n",
    "acc = metrics.accuracy_score(y_pred, y_valid)\n",
    "print(f\"knn accuracy: {round(acc,2)}\")\n",
    "\n",
    "# gaussian naive bayes\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred = gnb.predict(X_valid)\n",
    "acc = metrics.accuracy_score(y_pred, y_valid)\n",
    "print(f\"gaussian naive bayes accuracy: {round(acc,2)}\")\n",
    "\n",
    "# random forests\n",
    "rf = RandomForestClassifier(n_estimators=200) # most accurate in her example\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_valid)\n",
    "acc = metrics.accuracy_score(y_pred, y_valid)\n",
    "print(f\"random forest accuracy: {round(acc,2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.79\n",
      "mean: 0.64\n",
      "mean: 0.8\n",
      "mean: 0.64\n",
      "mean: 0.72\n",
      "mean: 0.78\n",
      "mean: 0.82\n",
      "                     CV Mean   Std\n",
      "Linear Svm              0.79  0.04\n",
      "Radial Svm              0.64  0.07\n",
      "Logistic Regression     0.80  0.03\n",
      "KNN                     0.64  0.07\n",
      "Decision Tree           0.72  0.09\n",
      "Naive Bayes             0.78  0.02\n",
      "Random Forest           0.82  0.04\n"
     ]
    }
   ],
   "source": [
    "# kaggle titanic example (pt3)\n",
    "\n",
    "#------------------\n",
    "# cross validation\n",
    "#------------------\n",
    "\n",
    "# cross validation\n",
    "    # https://towardsdatascience.com/cross-validation-70289113a072\n",
    "    # assess how well a model will generalize to an independent data set\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=22)\n",
    "classifiers = [\n",
    "    'Linear Svm', 'Radial Svm', 'Logistic Regression', 'KNN', 'Decision Tree', \n",
    "    'Naive Bayes' , 'Random Forest'\n",
    "]\n",
    "models = [\n",
    "    svm.SVC(kernel='linear', gamma='scale'), svm.SVC(kernel='rbf', gamma='scale'), \n",
    "    LogisticRegression(solver='liblinear'), KNeighborsClassifier(n_neighbors=9), \n",
    "    DecisionTreeClassifier(), GaussianNB(), RandomForestClassifier(n_estimators=100)\n",
    "]\n",
    "# results\n",
    "cv_mean = []\n",
    "cv_std = []\n",
    "cv_acc = []\n",
    "# cv\n",
    "for m in models:\n",
    "    model = m\n",
    "    cv_result = cross_val_score(model, X, y, cv=kfold, scoring='accuracy')\n",
    "    cv_mean.append(cv_result.mean())\n",
    "    cv_std.append(cv_result.std())\n",
    "    cv_acc.append(cv_result)\n",
    "    print(f\"mean: {round(cv_result.mean(),2)}\")\n",
    "    \n",
    "df_results = pd.DataFrame({'CV Mean': cv_mean, 'Std': cv_std}, index=classifiers)\n",
    "df_results = df_results.applymap(lambda x: round(x, 2))\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaggle titanic example (pt4)\n",
    "\n",
    "#------------------\n",
    "# confusion matrix\n",
    "#------------------\n",
    "\n",
    "# HEATMAP IS BROKEN FOR MATPLOTLIB 3.1.1\n",
    "\n",
    "f, ax = plt.subplots(3,3, figsize=(12,10))\n",
    "y_pred = cross_val_predict(svm.SVC(kernel='linear', gamma='scale'),X,y,cv=10)\n",
    "sns.heatmap(confusion_matrix(y,y_pred), ax=ax[0,0], annot=True,fmt='2.0f')\n",
    "ax[0,0].set_title('Linear SVM')\n",
    "\n",
    "y_pred = cross_val_predict(svm.SVC(kernel='rbf', gamma='scale'),X,y,cv=10)\n",
    "sns.heatmap(confusion_matrix(y,y_pred), ax=ax[0,1], annot=True,fmt='2.0f')\n",
    "ax[0,1].set_title('Radical SVM')\n",
    "\n",
    "y_pred = cross_val_predict(KNeighborsClassifier(n_neighbors=9) ,X,y,cv=10)\n",
    "sns.heatmap(confusion_matrix(y,y_pred), ax=ax[0,2], annot=True,fmt='2.0f')\n",
    "ax[0,2].set_title('KNN')\n",
    "\n",
    "y_pred = cross_val_predict(LogisticRegression(solver='liblinear') ,X,y,cv=10)\n",
    "sns.heatmap(confusion_matrix(y,y_pred), ax=ax[1,0], annot=True,fmt='2.0f')\n",
    "ax[1,0].set_title('Logistic Regression')\n",
    "\n",
    "y_pred = cross_val_predict(RandomForestClassifier(n_estimators=100) ,X,y,cv=10)\n",
    "sns.heatmap(confusion_matrix(y,y_pred), ax=ax[1,1], annot=True,fmt='2.0f')\n",
    "ax[1,1].set_title('Random Forest')\n",
    "\n",
    "y_pred = cross_val_predict(DecisionTreeClassifier() ,X,y,cv=10)\n",
    "sns.heatmap(confusion_matrix(y,y_pred), ax=ax[1,2], annot=True,fmt='2.0f')\n",
    "ax[1,2].set_title('Decision Tree')\n",
    "\n",
    "y_pred = cross_val_predict(GaussianNB() ,X,y,cv=10)\n",
    "sns.heatmap(confusion_matrix(y,y_pred), ax=ax[2,0], annot=True,fmt='2.0f')\n",
    "ax[2,0].set_title('Naive Bayes')\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
